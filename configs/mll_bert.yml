# 任务类型，有如下可选项：mlc; mll; mlm; ner; tg;
task: mll_pt
device: cuda
# 项目基本信息：用于项目整体组织管理和wandb饰演管理
project_info:
  # 项目名称
  project_name: "ysh_s2"
  # 实验分组：实验分组管理，相似实验进行组内对比
  group_name: "21label"
  # 本次运行的名称
  run_name: "Roberta"
  # 项目所在位置
  project_directory: "/data/wangfei/project/ysh_s2/"

# 模型训练配置
train:
  # 模型名称:text_cnn, bert
  model_type: bert
  # loss选择
  loss: base
  # early_stopping的patience
  early_stop: 5
  # 可选：micro avg;  macro avg;  weighted avg;
  metric_average: macro avg
  attention_dropout: 0.15
  hidden_dropout: 0.15

# Transformers TrainingArguments
train_arguments:
  do_train: True
  do_eval: True
  do_predict: True
  # checkpoints, 此处不填，会在训练文件中重新编辑
  output_dir: ""
  learning_rate: 0.00002
  per_device_eval_batch_size: 32
  per_device_train_batch_size: 32
  seed: 2022
  num_train_epochs: 30
  load_best_model_at_end: True
  evaluation_strategy: steps
  eval_steps: 1000
  save_steps: 1000
  # 默认开启混合精度训练
  fp16: True

# Transformers DataTrainingArguments
data_training_arguments:
  # The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,
  #  sequences shorter will be padded.
  max_seq_length: 128
  # Overwrite the cached preprocessed datasets or not.
  overwrite_cache: False
  #  Whether to pad all samples to `max_seq_length`.If False,
  #  will pad the samples dynamically when batching to the maximum length in the batch.
  pad_to_max_length: max_length
  #  A csv or a json file containing the training data.
  # /tmp/00_wangfei/data/ysh_s2/0629_split_model/role_train.json
  train_file: "/tmp/00_wangfei/data/ysh_s2/0829/train.json"
  #  A csv or a json file containing the validation data.
  validation_file: "/tmp/00_wangfei/data/ysh_s2/0829/dev.json"
  #  A csv or a json file containing the test data.
  test_file: "/tmp/00_wangfei/data/ysh_s2/0829/test.json"

# 预训练模型配置
pre_train_model:
  # tokenizer（分词器）存放位置
  # /data/wangfei/pre_model/chinese/bert_wwm_ext/
  # /data/wangfei/pre_model/chinese/robert_L8_512
  # /data/wangfei2/output/paddle2torch/torch/ernie_medium
  # /data/wangfei/pre_model/chinese_roberta_wwm_ext_pytorch/
  # "/data/wangfei/pre_model/ernie_3_medium_hf"
  tokenizer: "/data/wangfei/pre_model/chinese_roberta_wwm_ext_pytorch/"
  # 预训练模型存放位置
  model: "/data/wangfei/pre_model/chinese_roberta_wwm_ext_pytorch/"

# 日志配置（详情参见loguru官方文档：https://github.com/Delgan/loguru）
log:
  # 日志文件, 此处不填，会在训练文件中重新编辑
  file: ""
  # 日志分割方式
  rotation: "00:00:00"
  # 日志文件压缩方式
  compression: zip

# Send Message
message:
  feishu_bot:
    app_id: "cli_a164ead23d30d00d"
    app_secret: "zgZ40H0BnoQRQKLe2y0HoeDIRaAqEmzh"
    to_user_phone: "18684531169"